{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "770edaf2",
   "metadata": {},
   "source": [
    "\n",
    "# Friends Scripts — Preprocessing & Feature Extraction\n",
    "\n",
    "This notebook prepares linguistic, semantic, stylistic, and structural features from the Friends TV series scripts for Exceptional Model Mining (EMM). The target variable is the per-episode IMDB rating, and we craft predictors from the scripts, metadata, and derived analyses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880faa72",
   "metadata": {},
   "source": [
    "\n",
    "## Notebook Roadmap\n",
    "\n",
    "1. **Environment Setup** – install and import dependencies.\n",
    "2. **Load Data** – inspect dialogue and metadata tables.\n",
    "3. **Text Preprocessing** – clean, normalize, tokenize, and lemmatize dialogue while preserving negations.\n",
    "4. **Feature Engineering**\n",
    "   - Lexical and stylistic metrics\n",
    "   - Sentiment and emotion profiling\n",
    "   - Topic modeling & semantic features\n",
    "   - Humor indicators\n",
    "   - Complexity & readability\n",
    "   - Character interactions\n",
    "   - Metadata-derived attributes\n",
    "5. **Feature Aggregation** – consolidate all episode-level features and merge with IMDB ratings.\n",
    "6. **Exploratory Visualizations** – histograms, bar charts, heatmaps for sanity checks.\n",
    "7. **Export** – save the feature matrix for downstream EMM modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05481289",
   "metadata": {},
   "source": [
    "\n",
    "> **Tip:** Execute the notebook step-by-step. Some stages (e.g., transformer-based sentiment analysis or BERTopic) are computationally intensive—consider caching intermediate results when iterating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a9e04d",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "Install external libraries (run once per environment). Feel free to comment out packages you already have installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2682e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on a clean environment, uncomment the cell below.\n",
    "# !pip install -q pandas numpy nltk spacy textstat transformers datasets bertopic gensim torch networkx seaborn matplotlib\n",
    "# !python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e1cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations, tee\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "from textstat import flesch_kincaid_grade, gunning_fog\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ensure negations are preserved in stopword list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for neg in ['no', 'not', \"n't\", 'nor']:\n",
    "    if neg in stop_words:\n",
    "        stop_words.remove(neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ced673",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Load Data\n",
    "\n",
    "We use two CSV files:\n",
    "\n",
    "* `friends_all_episodes_clean.csv` – cleaned dialogue with speakers, seasons, episodes, etc.\n",
    "* `friends_episodes_v3.csv` – metadata including IMDB ratings.\n",
    "\n",
    "The loading cell below assumes the CSVs live in the repository root. Adjust `DATA_DIR` if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = '.'\n",
    "DIALOGUE_PATH = os.path.join(DATA_DIR, 'friends_all_episodes_clean.csv')\n",
    "META_PATH = os.path.join(DATA_DIR, 'friends_episodes_v3.csv')\n",
    "\n",
    "dialogue_df = pd.read_csv(DIALOGUE_PATH)\n",
    "meta_df = pd.read_csv(META_PATH)\n",
    "\n",
    "print('Dialogue shape:', dialogue_df.shape)\n",
    "print('Metadata shape:', meta_df.shape)\n",
    "dialogue_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7488260f",
   "metadata": {},
   "source": [
    "\n",
    "### Standardize Episode Identifiers\n",
    "\n",
    "Different datasets may use different column names. The helper below harmonizes identifiers and constructs a global episode index for merging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75250e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPISODE_ID_COLS = {\n",
    "    'season': ['season', 'season_num', 'Season', 'season_number'],\n",
    "    'episode': ['episode', 'episode_num', 'episode_number', 'Episode', 'EpisodeNumber', 'Episode_num'],\n",
    "    'title': ['title', 'episode_title', 'EpisodeTitle', 'episode_name']\n",
    "}\n",
    "\n",
    "\n",
    "def find_column(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(f\"None of the candidate columns {candidates} found in DataFrame\")\n",
    "\n",
    "\n",
    "season_col = find_column(dialogue_df, EPISODE_ID_COLS['season'])\n",
    "episode_col = find_column(dialogue_df, EPISODE_ID_COLS['episode'])\n",
    "\n",
    "if any(col in dialogue_df.columns for col in EPISODE_ID_COLS['title']):\n",
    "    title_col = find_column(dialogue_df, EPISODE_ID_COLS['title'])\n",
    "else:\n",
    "    title_col = None\n",
    "\n",
    "# Harmonize metadata as well\n",
    "meta_season_col = find_column(meta_df, EPISODE_ID_COLS['season'])\n",
    "meta_episode_col = find_column(meta_df, EPISODE_ID_COLS['episode'])\n",
    "meta_title_col = find_column(meta_df, EPISODE_ID_COLS['title']) if any(col in meta_df.columns for col in EPISODE_ID_COLS['title']) else None\n",
    "\n",
    "# Standard columns\n",
    "for df, s_col, e_col in [\n",
    "    (dialogue_df, season_col, episode_col),\n",
    "    (meta_df, meta_season_col, meta_episode_col)\n",
    "]:\n",
    "    df.rename(columns={s_col: 'season', e_col: 'episode'}, inplace=True)\n",
    "\n",
    "if title_col:\n",
    "    dialogue_df.rename(columns={title_col: 'episode_title'}, inplace=True)\n",
    "if meta_title_col:\n",
    "    meta_df.rename(columns={meta_title_col: 'episode_title'}, inplace=True)\n",
    "\n",
    "# Create a global episode index (season-episode formatted)\n",
    "dialogue_df['episode_code'] = dialogue_df['season'].astype(int).astype(str).str.zfill(2) + 'x' + dialogue_df['episode'].astype(int).astype(str).str.zfill(2)\n",
    "meta_df['episode_code'] = meta_df['season'].astype(int).astype(str).str.zfill(2) + 'x' + meta_df['episode'].astype(int).astype(str).str.zfill(2)\n",
    "\n",
    "# Add global index\n",
    "meta_df = meta_df.sort_values(['season', 'episode']).reset_index(drop=True)\n",
    "meta_df['episode_global_number'] = meta_df.index + 1\n",
    "\n",
    "print(dialogue_df[['season', 'episode', 'episode_code']].head())\n",
    "print(meta_df[['season', 'episode', 'episode_code', 'episode_global_number']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469f155",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Text Preprocessing\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Lowercase text and strip stage directions `[Scene: ...]` or parenthetical actions.\n",
    "2. Remove speaker prefixes (e.g., `ROSS:`) and filler utterances `(laughs)`, `(uh)`, etc.\n",
    "3. Tokenize using spaCy, remove stopwords (preserving negations), and lemmatize tokens.\n",
    "4. Aggregate line-level tokens per episode into documents.\n",
    "\n",
    "We keep both the cleaned text and token lists for downstream features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STAGE_DIRECTION_PATTERN = re.compile(r'\\[[^\\]]+\\]|\\([^\\)]*\\)')\n",
    "SPEAKER_PATTERN = re.compile(r'^[A-Z][A-Z\\s]+:')\n",
    "FILLERS = {\n",
    "    'uh', 'um', 'erm', 'hmm', 'mm', 'ah', 'oh', 'huh', 'hahaha',\n",
    "    'haha', 'lol', 'hehe', 'hmmm', 'huh', 'mmm', 'hahaha', 'hahah'\n",
    "}\n",
    "LAUGHTER_TOKENS = {'laugh', 'laughs', 'laughter', 'giggle', 'giggles'}\n",
    "LAUGHTER_REGEX = re.compile(r'\b(laughs?|giggles?|chuckles?|snickers?|funny)\b', re.IGNORECASE)\n",
    "\n",
    "catchphrases = [\n",
    "    \"how you doin'\", 'we were on a break', 'could i be', 'smelly cat',\n",
    "    'oh my god', 'my sandwich', \"he's her lobster\", 'i know!',\n",
    "]\n",
    "catchphrase_matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "catchphrase_patterns = [nlp.make_doc(phrase) for phrase in catchphrases]\n",
    "catchphrase_matcher.add('CATCHPHRASE', catchphrase_patterns)\n",
    "\n",
    "\n",
    "filler_phrase_patterns = [\n",
    "    re.compile(pattern, re.IGNORECASE)\n",
    "    for pattern in [\n",
    "        r'\b(uh|um|erm|hmm|huh|mm-hmm|mmm)\b',\n",
    "        r'\b(lol|haha+h+|hehe+h+|ahh+)\b',\n",
    "        r'\b(you know|i mean)\b'\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "def clean_dialogue_text(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = STAGE_DIRECTION_PATTERN.sub(' ', text)\n",
    "    text = SPEAKER_PATTERN.sub(' ', text)\n",
    "    for pattern in filler_phrase_patterns:\n",
    "        text = pattern.sub(' ', text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9'\\s]\", ' ', text)  # keep contractions\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def tokenize_and_lemmatize(text: str):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        if token.is_space or token.is_punct:\n",
    "            continue\n",
    "        if token.text in stop_words:\n",
    "            continue\n",
    "        if token.lemma_ == '-PRON-':\n",
    "            lemma = token.text.lower()\n",
    "        else:\n",
    "            lemma = token.lemma_.lower()\n",
    "        if lemma and lemma not in stop_words and lemma not in FILLERS:\n",
    "            tokens.append(token.text.lower())\n",
    "            lemmas.append(lemma)\n",
    "    return tokens, lemmas\n",
    "\n",
    "\n",
    "def preprocess_row(row):\n",
    "    cleaned = clean_dialogue_text(row.get('line') or row.get('dialogue') or row.get('quote') or '')\n",
    "    tokens, lemmas = tokenize_and_lemmatize(cleaned)\n",
    "    sentences = sent_tokenize(cleaned)\n",
    "    return pd.Series({\n",
    "        'clean_text': cleaned,\n",
    "        'tokens': tokens,\n",
    "        'lemmas': lemmas,\n",
    "        'num_words': len(tokens),\n",
    "        'num_lemmas': len(lemmas),\n",
    "        'num_sentences': len(sentences),\n",
    "        'sentence_lengths': [len(nlp(sent).to_array(['ORTH'])) for sent in sentences if sent.strip()]\n",
    "    })\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "preprocessed_df = dialogue_df.join(dialogue_df.apply(preprocess_row, axis=1))\n",
    "preprocessed_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c3823",
   "metadata": {},
   "source": [
    "\n",
    "### Aggregate Dialogue Per Episode\n",
    "\n",
    "We create a corpus where each episode is represented by its concatenated lemmatized text. This will feed lexical, sentiment, topic, and other feature computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d38767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify character column\n",
    "character_col_candidates = ['character', 'speaker', 'Person', 'person', 'name']\n",
    "character_col = find_column(preprocessed_df, character_col_candidates)\n",
    "preprocessed_df.rename(columns={character_col: 'character'}, inplace=True)\n",
    "\n",
    "preprocessed_df['character'] = preprocessed_df['character'].str.title().str.strip()\n",
    "preprocessed_df['line_id'] = np.arange(len(preprocessed_df))\n",
    "\n",
    "# Episode-level aggregation\n",
    "agg_funcs = {\n",
    "    'clean_text': lambda x: ' '.join(x),\n",
    "    'lemmas': lambda x: list(np.concatenate(x.values)),\n",
    "    'tokens': lambda x: list(np.concatenate(x.values)),\n",
    "    'num_words': 'sum',\n",
    "    'num_sentences': 'sum',\n",
    "    'sentence_lengths': lambda x: list(np.concatenate(x.values)),\n",
    "}\n",
    "\n",
    "episode_docs = preprocessed_df.groupby('episode_code').agg(agg_funcs)\n",
    "\n",
    "episode_docs['episode_length'] = episode_docs['num_words']\n",
    "\n",
    "episode_docs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb251e7",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Lexical & Stylistic Features\n",
    "\n",
    "We compute:\n",
    "\n",
    "* **Total word count** per episode\n",
    "* **Average sentence length** (tokens per sentence)\n",
    "* **Type-token ratio** (unique lemmas / total lemmas)\n",
    "* **Average words per line**\n",
    "* **Character speaking proportions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_lexical_features(df_lines, episode_docs):\n",
    "    lexical_features = episode_docs[['num_words']].rename(columns={'num_words': 'total_words'})\n",
    "\n",
    "    lexical_features['avg_sentence_length'] = episode_docs.apply(\n",
    "        lambda row: np.mean([len(nlp(sent)) for sent in sent_tokenize(row['clean_text'])]) if row['num_sentences'] > 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    lexical_features['type_token_ratio'] = episode_docs['lemmas'].apply(lambda lemmas: len(set(lemmas)) / (len(lemmas) or 1))\n",
    "\n",
    "    avg_words_per_line = df_lines.groupby('episode_code')['num_words'].mean().rename('avg_words_per_line')\n",
    "    lexical_features = lexical_features.join(avg_words_per_line)\n",
    "\n",
    "    # Character speaking proportions\n",
    "    char_counts = df_lines.groupby(['episode_code', 'character']).size().rename('line_count').reset_index()\n",
    "    char_totals = char_counts.groupby('episode_code')['line_count'].transform('sum')\n",
    "    char_counts['line_pct'] = char_counts['line_count'] / char_totals\n",
    "\n",
    "    major_characters = ['Rachel', 'Ross', 'Monica', 'Chandler', 'Joey', 'Phoebe']\n",
    "    char_pivot = char_counts.pivot_table(index='episode_code', columns='character', values='line_pct', fill_value=0)\n",
    "    for character in major_characters:\n",
    "        if character not in char_pivot.columns:\n",
    "            char_pivot[character] = 0.0\n",
    "    char_pivot = char_pivot[[c for c in char_pivot.columns if c in major_characters]]\n",
    "    char_pivot.columns = [f'line_pct_{c.lower()}' for c in char_pivot.columns]\n",
    "\n",
    "    lexical_features = lexical_features.join(char_pivot, how='left')\n",
    "    lexical_features.fillna(0, inplace=True)\n",
    "    return lexical_features\n",
    "\n",
    "\n",
    "lexical_features = compute_lexical_features(preprocessed_df, episode_docs)\n",
    "lexical_features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7545f0b9",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Sentiment & Emotion Features\n",
    "\n",
    "* **Sentiment:** Use `cardiffnlp/twitter-roberta-base-sentiment` to score each line, then compute mean and variance per episode.\n",
    "* **Emotion:** Map lemmas to emotions via the NRC Emotion Lexicon (anger, joy, sadness, surprise, disgust, fear) and aggregate proportions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1281a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sentiment pipeline (transformer)\n",
    "sentiment_analyzer = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment')\n",
    "\n",
    "\n",
    "def score_sentiment(text):\n",
    "    if not text:\n",
    "        return {'neg': 0.0, 'neu': 0.0, 'pos': 0.0}\n",
    "    result = sentiment_analyzer(text, truncation=True)\n",
    "    scores = defaultdict(float)\n",
    "    for res in result:\n",
    "        label = res['label'].lower()\n",
    "        scores[label] += res['score']\n",
    "    return scores\n",
    "\n",
    "\n",
    "sentiment_scores = preprocessed_df['clean_text'].apply(score_sentiment)\n",
    "sentiment_df = pd.DataFrame(list(sentiment_scores))\n",
    "preprocessed_df = preprocessed_df.join(sentiment_df)\n",
    "\n",
    "sentiment_episode = preprocessed_df.groupby('episode_code')[['neg', 'neu', 'pos']].agg(['mean', 'var']).fillna(0)\n",
    "sentiment_episode.columns = ['_'.join(col).strip() for col in sentiment_episode.columns.values]\n",
    "\n",
    "sentiment_episode.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc642d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load NRC Emotion Lexicon\n",
    "import requests\n",
    "\n",
    "NRC_URL = 'https://raw.githubusercontent.com/nductrin/NRC-Emotion-Lexicon-v0.92/master/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "lexicon_path = 'nrc_lexicon.txt'\n",
    "if not os.path.exists(lexicon_path):\n",
    "    response = requests.get(NRC_URL)\n",
    "    response.raise_for_status()\n",
    "    with open(lexicon_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "nrc_df = pd.read_csv(\n",
    "    lexicon_path,\n",
    "    names=['word', 'emotion', 'association'],\n",
    "    sep='\t'\n",
    ")\n",
    "\n",
    "nrc_df = nrc_df[nrc_df['association'] == 1]\n",
    "emotion_map = nrc_df.groupby('word')['emotion'].apply(list).to_dict()\n",
    "\n",
    "def map_emotions(lemmas):\n",
    "    counter = Counter()\n",
    "    for lemma in lemmas:\n",
    "        if lemma in emotion_map:\n",
    "            counter.update(emotion_map[lemma])\n",
    "    total = sum(counter.values()) or 1\n",
    "    return {emotion: counter.get(emotion, 0) / total for emotion in ['anger', 'joy', 'sadness', 'surprise', 'disgust', 'fear']}\n",
    "\n",
    "emotion_features = episode_docs['lemmas'].apply(map_emotions)\n",
    "emotion_features = pd.DataFrame(list(emotion_features), index=episode_docs.index)\n",
    "emotion_features.columns = [f'emotion_{col}' for col in emotion_features.columns]\n",
    "emotion_features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4a4094",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Topic Modeling & Semantic Features\n",
    "\n",
    "We compute semantic features via BERTopic (can switch to LDA if resources are limited). Steps:\n",
    "\n",
    "1. Fit BERTopic on episode documents.\n",
    "2. Extract dominant topic per episode and topic probability scores.\n",
    "3. Count frequencies for selected high-level keywords (`love`, `job`, `marriage`, etc.).\n",
    "4. Perform Named Entity Recognition (NER) with spaCy to count salient entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc7c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare documents for BERTopic (lemmatized strings)\n",
    "episode_docs['lemma_text'] = episode_docs['lemmas'].apply(lambda lemmas: ' '.join(lemmas))\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "bertopic_model = BERTopic(vectorizer_model=vectorizer_model, language='english', calculate_probabilities=True)\n",
    "\n",
    "topics, probs = bertopic_model.fit_transform(episode_docs['lemma_text'])\n",
    "episode_docs['bertopic_topic'] = topics\n",
    "episode_docs['bertopic_score'] = [prob.max() if prob is not None else np.nan for prob in probs]\n",
    "\n",
    "topic_info = bertopic_model.get_topic_info()\n",
    "topic_info.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keyword frequency counts\n",
    "keywords = ['love', 'job', 'marriage', 'break', 'date', 'baby', 'wedding', 'career', 'friend']\n",
    "\n",
    "def count_keywords(lemmas):\n",
    "    counter = Counter(lemmas)\n",
    "    return {f'kw_{kw}': counter.get(kw, 0) for kw in keywords}\n",
    "\n",
    "keyword_features = episode_docs['lemmas'].apply(count_keywords)\n",
    "keyword_features = pd.DataFrame(list(keyword_features), index=episode_docs.index)\n",
    "keyword_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5592a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Named entity extraction\n",
    "NER_COLUMNS = ['GPE', 'LOC', 'PERSON', 'ORG', 'EVENT', 'WORK_OF_ART']\n",
    "\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    counter = Counter()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in NER_COLUMNS:\n",
    "            counter[f'ner_{ent.label_.lower()}'] += 1\n",
    "    return counter\n",
    "\n",
    "ner_features = episode_docs['clean_text'].apply(extract_named_entities)\n",
    "ner_features = pd.DataFrame(list(ner_features), index=episode_docs.index)\n",
    "ner_features.fillna(0, inplace=True)\n",
    "ner_features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac95a6bb",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Humor-Related Features\n",
    "\n",
    "* Count lines containing laughter markers.\n",
    "* Detect occurrences of canonical catchphrases.\n",
    "* Optional: placeholder for sarcasm scores (extend with a dedicated model if available).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27853a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Laughter counts per line\n",
    "preprocessed_df['has_laughter'] = preprocessed_df['clean_text'].str.contains(LAUGHTER_REGEX).fillna(False)\n",
    "\n",
    "# Catchphrase detection using spaCy PhraseMatcher\n",
    "\n",
    "def count_catchphrases(text):\n",
    "    doc = nlp.make_doc(text)\n",
    "    matches = catchphrase_matcher(doc)\n",
    "    return len(matches)\n",
    "\n",
    "preprocessed_df['catchphrase_count'] = preprocessed_df['clean_text'].apply(count_catchphrases)\n",
    "\n",
    "humor_episode = preprocessed_df.groupby('episode_code').agg({\n",
    "    'has_laughter': 'sum',\n",
    "    'catchphrase_count': 'sum'\n",
    "})\n",
    "humor_episode.rename(columns={\n",
    "    'has_laughter': 'laughter_line_count',\n",
    "    'catchphrase_count': 'catchphrase_count'\n",
    "}, inplace=True)\n",
    "\n",
    "# Placeholder for sarcasm score\n",
    "preprocessed_df['sarcasm_score'] = np.nan  # Replace with actual model output if available\n",
    "sarcasm_episode = preprocessed_df.groupby('episode_code')['sarcasm_score'].mean().rename('sarcasm_score_mean')\n",
    "\n",
    "humor_features = humor_episode.join(sarcasm_episode)\n",
    "humor_features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e98b4",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Complexity Features\n",
    "\n",
    "* **Flesch-Kincaid** and **Gunning Fog** readability scores per episode using `textstat`.\n",
    "* **Syntactic complexity** via average dependency tree depth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7608819",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_readability(text):\n",
    "    if not text.strip():\n",
    "        return pd.Series({'flesch_kincaid': np.nan, 'gunning_fog': np.nan})\n",
    "    return pd.Series({\n",
    "        'flesch_kincaid': flesch_kincaid_grade(text),\n",
    "        'gunning_fog': gunning_fog(text)\n",
    "    })\n",
    "\n",
    "\n",
    "readability_features = episode_docs['clean_text'].apply(compute_readability)\n",
    "\n",
    "\n",
    "def dependency_depth(doc):\n",
    "    depths = [len(list(token.ancestors)) for token in doc]\n",
    "    return np.mean(depths) if depths else np.nan\n",
    "\n",
    "\n",
    "syntax_features = episode_docs['clean_text'].apply(lambda text: dependency_depth(nlp(text))).rename('avg_dependency_depth')\n",
    "\n",
    "complexity_features = readability_features.join(syntax_features)\n",
    "complexity_features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdf658d",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Character Interaction Features\n",
    "\n",
    "We explore conversational dynamics:\n",
    "\n",
    "* **Turn-taking matrix:** counts transitions between speakers.\n",
    "* **Co-occurrence density:** overall interaction richness.\n",
    "* **Key pair interactions:** counts for iconic pairs (e.g., Chandler–Joey).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38389179",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iconic_pairs = [\n",
    "    ('Chandler', 'Joey'),\n",
    "    ('Ross', 'Rachel'),\n",
    "    ('Monica', 'Chandler'),\n",
    "    ('Phoebe', 'Joey'),\n",
    "    ('Ross', 'Monica'),\n",
    "    ('Rachel', 'Monica')\n",
    "]\n",
    "\n",
    "\n",
    "def compute_interactions(df):\n",
    "    interactions = defaultdict(lambda: defaultdict(int))\n",
    "    pair_counter = Counter()\n",
    "    speakers = df['character'].fillna('Unknown').tolist()\n",
    "    for prev, curr in zip(speakers, speakers[1:]):\n",
    "        if prev == curr:\n",
    "            continue\n",
    "        interactions[prev][curr] += 1\n",
    "        pair = tuple(sorted((prev, curr)))\n",
    "        pair_counter[pair] += 1\n",
    "    matrix = pd.DataFrame(interactions).fillna(0)\n",
    "    total_turns = sum(pair_counter.values())\n",
    "    unique_pairs = len(pair_counter)\n",
    "    density = total_turns / (len(set(speakers)) ** 2 or 1)\n",
    "    features = {\n",
    "        'total_turns': total_turns,\n",
    "        'unique_pairs': unique_pairs,\n",
    "        'interaction_density': density\n",
    "    }\n",
    "    for pair in iconic_pairs:\n",
    "        features[f'interactions_{pair[0].lower()}_{pair[1].lower()}'] = pair_counter.get(tuple(sorted(pair)), 0)\n",
    "    return features\n",
    "\n",
    "interaction_features = preprocessed_df.sort_values('line_id').groupby('episode_code').apply(compute_interactions)\n",
    "interaction_features = pd.DataFrame(list(interaction_features), index=episode_docs.index)\n",
    "interaction_features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c961434",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Meta Features\n",
    "\n",
    "Derive structured attributes from metadata:\n",
    "\n",
    "* Episode number within season and globally\n",
    "* Season premiere/finale flags\n",
    "* Season encoded as categorical dummies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57302392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_features = meta_df.set_index('episode_code')[['season', 'episode', 'episode_global_number']]\n",
    "\n",
    "# Premiere / finale flags\n",
    "meta_features['is_season_premiere'] = meta_features.groupby('season')['episode'].transform(lambda x: x == x.min()).astype(int)\n",
    "meta_features['is_season_finale'] = meta_features.groupby('season')['episode'].transform(lambda x: x == x.max()).astype(int)\n",
    "\n",
    "season_dummies = pd.get_dummies(meta_features['season'], prefix='season')\n",
    "meta_features = meta_features.join(season_dummies)\n",
    "\n",
    "meta_features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b39422",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Consolidate Feature Matrix\n",
    "\n",
    "Combine all feature tables into a single DataFrame indexed by `episode_code`, then merge with IMDB ratings. Missing values are filled conservatively with zeros or column means depending on context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_frames = [\n",
    "    lexical_features,\n",
    "    sentiment_episode,\n",
    "    emotion_features,\n",
    "    keyword_features,\n",
    "    ner_features,\n",
    "    humor_features,\n",
    "    complexity_features,\n",
    "    interaction_features,\n",
    "    meta_features\n",
    "]\n",
    "\n",
    "feature_matrix = pd.concat(feature_frames, axis=1)\n",
    "\n",
    "# Merge with BERTopic outputs separately to avoid duplicates\n",
    "feature_matrix = feature_matrix.join(episode_docs[['bertopic_topic', 'bertopic_score']], how='left')\n",
    "\n",
    "# Merge IMDB ratings / metadata\n",
    "rating_col_candidates = ['imdb_rating', 'imdb', 'imdb_rating_scaled', 'Rating', 'imdbRating']\n",
    "imdb_col = find_column(meta_df, rating_col_candidates)\n",
    "\n",
    "meta_target = meta_df.set_index('episode_code')[[imdb_col, 'episode_title', 'episode_global_number']]\n",
    "meta_target.rename(columns={imdb_col: 'imdb_rating'}, inplace=True)\n",
    "\n",
    "full_dataset = feature_matrix.join(meta_target, how='left')\n",
    "\n",
    "# Handle missing values\n",
    "numeric_cols = full_dataset.select_dtypes(include=[np.number]).columns\n",
    "full_dataset[numeric_cols] = full_dataset[numeric_cols].fillna(0)\n",
    "full_dataset['episode_title'] = full_dataset['episode_title'].fillna('Unknown')\n",
    "\n",
    "full_dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd105b",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Exploratory Visualizations\n",
    "\n",
    "Sanity-check feature distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d52006",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(full_dataset['total_words'], bins=20, kde=True)\n",
    "plt.title('Distribution of Episode Word Counts')\n",
    "plt.xlabel('Total Words')\n",
    "plt.ylabel('Episodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8799a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "char_cols = [col for col in full_dataset.columns if col.startswith('line_pct_')]\n",
    "full_dataset[char_cols].mean().sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('Average Speaking Share by Main Characters')\n",
    "plt.ylabel('Average % of Lines')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18825c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "interaction_cols = [col for col in full_dataset.columns if col.startswith('interactions_')]\n",
    "sns.heatmap(full_dataset[interaction_cols], cmap='viridis')\n",
    "plt.title('Character Pair Interactions per Episode')\n",
    "plt.xlabel('Pair')\n",
    "plt.ylabel('Episode Index')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83831655",
   "metadata": {},
   "source": [
    "\n",
    "## 13. Export Feature Matrix\n",
    "\n",
    "Save the processed dataset for downstream Exceptional Model Mining experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a735cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUTPUT_PATH = 'friends_episode_feature_matrix.csv'\n",
    "full_dataset.to_csv(OUTPUT_PATH, index=True)\n",
    "print(f'Saved feature matrix to {OUTPUT_PATH}. Rows: {len(full_dataset)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8763a76",
   "metadata": {},
   "source": [
    "\n",
    "### Next Steps\n",
    "\n",
    "* Run preliminary EMM analyses (e.g., pattern mining on high-IMDB episodes).\n",
    "* Experiment with alternative sentiment / emotion models for robustness.\n",
    "* Integrate sarcasm detection when a reliable model is available.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
